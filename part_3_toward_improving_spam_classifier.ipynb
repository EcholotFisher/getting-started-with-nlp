{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward improving the spam classifier\n",
    "\n",
    "The notebook `part_1_spam_classifier` provided a baseline classifier, we will investigate here how it can be improved through feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the notebook `part_1_spam_classifier`, we will use data from `SMS Spam Collection v. 1` described as:\n",
    "\n",
    "> a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam.\n",
    "\n",
    "([source](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load useful librairies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Makingsure the required NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "spam_data = pd.read_csv(\n",
    "    \"./data/SMSSpamCollection.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    header=None,\n",
    "    delimiter=\"\\t\",\n",
    "    names=[\"target\", \"text\"],\n",
    ")\n",
    "\n",
    "# Encoding target variable\n",
    "spam_data[\"target\"] = np.where(spam_data[\"target\"] == \"spam\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper dive into the data\n",
    "\n",
    "The way to engineer features with text is similar to the one when working with numbers. \n",
    "\n",
    "To get an intuition of which parameters could add predictiveness, let's take a deeper look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Examples of spam SMS: \\n    {}\\n    {}\".format(\n",
    "        spam_data[spam_data.target == 1].sample(1).text.iloc[0],\n",
    "        spam_data[spam_data.target == 1].sample(1).text.iloc[0],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"\\nExamples of non-spam SMS: \\n    {}\\n    {}\\n\".format(\n",
    "        spam_data[spam_data.target == 0].sample(1).text.iloc[0],\n",
    "        spam_data[spam_data.target == 0].sample(1).text.iloc[0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam messages seem to be longer than ham ones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average message length:\")\n",
    "print(\"   Spam = {:.0f} characters\".format(np.mean([len(x) for x in spam_data[spam_data.target == 1].text])))\n",
    "print(\"   Non spam = {:.0f} characters\".format(np.mean([len(x) for x in spam_data[spam_data.target == 0].text])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and they are!\n",
    "\n",
    "Then, spam messages seem to contain more digits than ham ones. Let's check that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ==> Finding specific characteristics using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"URGENT This is our 2nd attempt to contact U. Your Â£900 prize from YESTERDAY is still awaiting collection. To claim CALL NOW 09061702893\"\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\d', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\D', example)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of digits:\")\n",
    "print(\"   Spam = {:.0f}\".format(np.mean([len(x) for x in list(spam_data[spam_data.target == 1].text.str.findall(r'\\d'))])))\n",
    "print(\"   Non spam = {:.1f}\".format(np.mean([len(x) for x in list(spam_data[spam_data.target == 0].text.str.findall(r'\\d'))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and they do :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring spam corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = spam_data[spam_data.target == 1].text.copy()\n",
    "# Have the spam corpus as a long string\n",
    "spam_corpus = ' '.join(spam_corpus.tolist())\n",
    "# Split this string into tokens\n",
    "spam_tokens = nltk.word_tokenize(spam_corpus.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_spam = nltk.FreqDist(spam_tokens)\n",
    "dist_spam_sorted = {k: v for k,v in sorted(dist_spam.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v) for k,v in dist_spam_sorted.items()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that among the most frequent tokens, are punctuation (\".\", \",\", \"!\") as well as very common words (\"a\", \"to\", \"the\"). These words are called \"stop words\" and we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english'))) + ['u', 'ur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_tokens = [x for x in spam_tokens if x.isalpha() and x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_spam = nltk.FreqDist(spam_tokens)\n",
    "dist_spam_sorted = {k: v for k,v in sorted(dist_spam.items(), key=lambda item: item[1], reverse=True)}\n",
    "[(k, v) for k,v in dist_spam_sorted.items()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring non spam corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_spam_corpus = spam_data[spam_data.target == 0].text.copy()\n",
    "non_spam_corpus = ' '.join(non_spam_corpus.tolist())\n",
    "non_spam_tokens = nltk.word_tokenize(non_spam_corpus.lower())\n",
    "\n",
    "non_spam_tokens = [x for x in non_spam_tokens if x.isalpha() and x not in stop_words]\n",
    "\n",
    "dist_non_spam = nltk.FreqDist(non_spam_tokens)\n",
    "dist_non_spam_sorted = {k: v for k,v in sorted(dist_non_spam.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v) for k,v in dist_non_spam_sorted.items()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note on stemming and lemmatization\n",
    "\n",
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"List listed lists listing listings.\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = nltk.word_tokenize(input1)\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in nltk.word_tokenize(input1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the Porter Stemmer Algorithm can be found here: https://tartarus.org/martin/PorterStemmer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'Multiply the numbers independently and count decimal points then, for the division, push the decimal places like i showed you.'\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = nltk.word_tokenize(example.lower())\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = set([x for x in example if x.isalpha() and x not in stop_words])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([porter.stem(t) for t in example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([WNlemma.lemmatize(t) for t in example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To take home\n",
    "\n",
    "Based on the analysis above, which features would you add to improve our spam classifier? Is the AUC score improving?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
